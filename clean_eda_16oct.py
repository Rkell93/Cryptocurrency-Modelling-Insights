# -*- coding: utf-8 -*-
"""Clean_EDA_16Oct

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16a8rj9UN7UwWD_m6GIwzmbGRnafwaIeb
"""



"""" Stationarity in LSTMs" 
https://datascience.stackexchange.com/questions/24800/time-series-prediction-using-lstms-importance-of-making-time-series-stationary
"""

import tensorflow as tf
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

#import matplotlib.pyplot as plt
#import seaborn as sns
#Use seaborn style defaults and set the default figure size
sns.set(rc={'figure.figsize':(20, 10)})

# Set seeds for numpy and tensorflow
tf.random.set_seed(12)
np.random.seed(12)

"""# Daily Data Set """

daily_data_url = "https://raw.githubusercontent.com/DonaldTurton/iLab2/master/Merged_datasets/bit_eth_xrp_daily.csv"
daily_data = pd.read_csv (daily_data_url)
daily_data

data = daily_data[["time",   "BTC_close", "BTC_vol",  "ETHER_close", "ETHER_vol",     "XRP_close", "XRP_vol" ]]
data

# Convert Date column to Date format.  
#data["time"] = pd.to_datetime(data["time"], dayfirst=True )  #, format ='%d/%m/%y', errors='ignore'
#Set Date as Index
#data = data.set_index('time')#.asfreq('d')
#data

"""# Data Distribution"""



"""# Correlation Plots

"""

#Correlation plot
corrMatrix = data.corr()
sns.heatmap(corrMatrix, cmap="YlGnBu", annot=True)
plt.title('Correlation Plot')

"""# Standard Scaling Plot"""

# Compute the same graph as above but standardise the data first
from sklearn import preprocessing

# Create the Scaler object
scaler = preprocessing.StandardScaler()

min_max_scaler = preprocessing.MinMaxScaler()

data_close_prices = data[['BTC_close', 'XRP_close', 'ETHER_close']]

data_close_prices = pd.DataFrame(min_max_scaler.fit_transform(data_close_prices))

data_close_prices.columns = ["BTC_close", "XRP_close", "ETHER_close"]

data_close_prices
plt.plot(data_close_prices['BTC_close'], label = "BTC") 
plt.plot(data_close_prices['ETHER_close'], label = "ETHER") 
plt.plot(data_close_prices['XRP_close'], label = "XRP") 
plt.title('Close prices - Standardised' )
plt.legend()
plt.show()

"""# Seasonal Decomposition

https://www.machinelearningplus.com/time-series/time-series-analysis-python/
"""

# Convert Date column to Date format.  
df = data 
df["time"] = pd.to_datetime(df["time"], dayfirst=True )  #, format ='%d/%m/%y', errors='ignore'

# Create variables for day of month 
df['Month'] = df['time'].dt.month_name()
df.Month = df.Month.astype('category') 
df['Month'] = df['Month'].cat.reorder_categories(['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'])


#Set Date as Index
df = df.set_index('time')#.asfreq('d')
print(df.info())
df

"""# Removing bubble"""

import matplotlib.dates as mdates
sns.boxplot(data= df, x='Month', y=df.BTC_close, order=['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'])
plt.ylabel('Daily Value')
plt.title("BTC Monthly Behaviour / May 2017 to August 2020")

import matplotlib.dates as mdates


# Subset data to March2018 - today. # COMMENT TO INSPECT ENTIRE DATA SET
df_2018_2020 = df['2018-03-01':'2020-08-02']


sns.boxplot(data= df_2018_2020, x='Month', y=df_2018_2020.BTC_close, order=['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'])
plt.ylabel('Daily Value')
plt.title("BTC Monthly Behaviour / March 2018 to August 2020")

"""## Test For Stationarity"""

from statsmodels.tsa.stattools import adfuller, kpss

# ADF Test
result = adfuller(df.BTC_close.values, autolag='AIC')
print(f'ADF Statistic: {result[0]}')
print(f'p-value: {result[1]}')
for key, value in result[4].items():
    print('Critial Values:')
    print(f'   {key}, {value}')

# KPSS Test
result = kpss(df.BTC_close.values, regression='c')
print('\nKPSS Statistic: %f' % result[0])
print('p-value: %f' % result[1])
for key, value in result[3].items():
    print('Critial Values:')
    print(f'   {key}, {value}')

""""ADF: where the null hypothesis is the time series possesses a unit root and is non-stationary. So, id the P-Value in ADH test is less than the significance level (0.05), you reject the null hypothesis."

"KPSS: is used to test for trend stationarity. The null hypothesis and the P-Value interpretation is just the opposite of ADH test." 

ADF: With a p-value > 0.05 we do not have enough evidence to reject the Ho of NON-stationarity
"""

# ADF Test
result = adfuller(df.ETHER_close.values, autolag='AIC')
print(f'ADF Statistic: {result[0]}')
print(f'p-value: {result[1]}')
for key, value in result[4].items():
    print('Critial Values:')
    print(f'   {key}, {value}')


# KPSS Test
result = kpss(df.ETHER_close.values, regression='c')
print('\nKPSS Statistic: %f' % result[0])
print('p-value: %f' % result[1])
for key, value in result[3].items():
    print('Critial Values:')
    print(f'   {key}, {value}')

# ADF Test
result = adfuller(df.XRP_close.values, autolag='AIC')
print(f'ADF Statistic: {result[0]}')
print(f'p-value: {result[1]}')
for key, value in result[4].items():
    print('Critial Values:')
    print(f'   {key}, {value}')

# KPSS Test
result = kpss(df.XRP_close.values, regression='c')
print('\nKPSS Statistic: %f' % result[0])
print('p-value: %f' % result[1])
for key, value in result[3].items():
    print('Critial Values:')
    print(f'   {key}, {value}')

"""XRP: with a p-value of 0.002> 0.05, there is wnough evidence to reject the Ho of Non-stattionarity

## Seasonal Decomp
"""

from pandas import read_csv
from matplotlib import pyplot
from statsmodels.tsa.seasonal import seasonal_decompose

#Bitcoin Seasonal Decompose
result = seasonal_decompose(df["BTC_close"], model='multiplicative')
result.plot()
pyplot.show()

#Ripple Seasonal Decompose
result = seasonal_decompose(df["XRP_close"], model='multiplicative')
result.plot()
pyplot.show()

#Ether Seasonal Decompose
result = seasonal_decompose(df["ETHER_close"], model='multiplicative')
result.plot()
pyplot.show()

"""## Autocorrelation and Partial correlation"""

from pandas.plotting import autocorrelation_plot
autocorrelation_plot(df['BTC_close'])
plt.title('BTC Autocorrelation plot')
plt.axhline(y=0.70, color='r', linestyle='-')
plt.axvline(x=30, color='r')
plt.xticks(np.arange(0, 1200, step=50))
plt.show()

autocorrelation_plot(df['XRP_close'])
plt.title('XRP Autocorrelation plot')
plt.axhline(y=0.70, color='r', linestyle='-')
plt.axvline(x=20, color='r')
plt.xticks(np.arange(0, 1200, step=50))
plt.show()

autocorrelation_plot(df['ETHER_close'])
plt.title('ETHER Autocorrelation plot')
plt.axhline(y=0.70, color='r', linestyle='-')
plt.axvline(x=45, color='r')
plt.xticks(np.arange(0, 1200, step=50))
plt.show()



from statsmodels.tsa.stattools import acf, pacf
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
# Calculate ACF and PACF upto 50 lags
# acf_50 = acf(df.value, nlags=50)
# pacf_50 = pacf(df.value, nlags=50)

# Draw Plot
fig, axes = plt.subplots(1,2,figsize=(16,3), dpi= 100)
plot_acf(df.BTC_close.tolist(), lags=50, ax=axes[0])
plot_pacf(df.BTC_close.tolist(), lags=50, ax=axes[1])

# Draw Plot
fig, axes = plt.subplots(1,2,figsize=(16,3), dpi= 100)
plot_acf(df.XRP_close.tolist(), lags=50, ax=axes[0])
plot_pacf(df.XRP_close.tolist(), lags=50, ax=axes[1])

# Draw Plot
fig, axes = plt.subplots(1,2,figsize=(16,3), dpi= 100)
plot_acf(df.ETHER_close.tolist(), lags=50, ax=axes[0])
plot_pacf(df.ETHER_close.tolist(), lags=50, ax=axes[1])

"""# Entropy
forecastability of a time series
https://en.wikipedia.org/wiki/Approximate_entropy
"""

def ApEn(U, m, r):
    """Compute Aproximate entropy"""
    def _maxdist(x_i, x_j):
        return max([abs(ua - va) for ua, va in zip(x_i, x_j)])

    def _phi(m):
        x = [[U[j] for j in range(i, i + m - 1 + 1)] for i in range(N - m + 1)]
        C = [len([1 for x_j in x if _maxdist(x_i, x_j) <= r]) / (N - m + 1.0) for x_i in x]
        return (N - m + 1.0)**(-1) * sum(np.log(C))

    N = len(U)
    return abs(_phi(m+1) - _phi(m))

print(ApEn(df.BTC_close, m=2, r=0.2*np.std(df.BTC_close)))  
print(ApEn(df.XRP_close, m=2, r=0.2*np.std(df.XRP_close)))  
print(ApEn(df.ETHER_close, m=2, r=0.2*np.std(df.ETHER_close)))

"""The more regular and repeatable patterns a time series has, the easier it is to forecast. The ‘Approximate Entropy’ can be used to quantify the regularity and unpredictability of fluctuations in a time series.

#Stationarised

https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/
"""

df

"""## Log"""

df =  df['2018-03-01':'2020-08-02']

BTC_log = np.log(df["BTC_close"])
plt.plot(BTC_log)

"""## Moving Average(Smoothing)"""

#moving_avg = pd.rolling_mean(BTC_log,12)
moving_avg = BTC_log.rolling(50).mean()
plt.plot(BTC_log)
plt.plot(moving_avg, color='red')

BTC_log_moving_avg_diff = BTC_log - moving_avg
BTC_log_moving_avg_diff.head(100)

BTC_log_moving_avg_diff.dropna(inplace=True) #drop Na
#test_stationarity(BTC_log_moving_avg_diff)

from statsmodels.tsa.stattools import adfuller, kpss

# ADF Test
result = adfuller(BTC_log_moving_avg_diff, autolag='AIC')
print(f'ADF Statistic: {result[0]}')
print(f'p-value: {result[1]}')
for key, value in result[4].items():
    print('Critial Values:')
    print(f'   {key}, {value}')

# KPSS Test
result = kpss(BTC_log_moving_avg_diff, regression='c')
print('\nKPSS Statistic: %f' % result[0])
print('p-value: %f' % result[1])
for key, value in result[3].items():
    print('Critial Values:')
    print(f'   {key}, {value}')

from pandas import read_csv
from matplotlib import pyplot
from statsmodels.tsa.seasonal import seasonal_decompose

#Bitcoin Seasonal Decompose
result = seasonal_decompose(BTC_log_moving_avg_diff)
result.plot()
pyplot.show()

"""## Diff"""

BTC_log_diff = BTC_log - BTC_log.shift(1)
plt.plot(BTC_log_diff)

BTC_log_diff.dropna(inplace=True)

# ADF Test
result = adfuller(BTC_log_diff, autolag='AIC')
print(f'ADF Statistic: {result[0]}')
print(f'p-value: {result[1]}')
for key, value in result[4].items():
    print('Critial Values:')
    print(f'   {key}, {value}')

# KPSS Test
result = kpss(BTC_log_diff, regression='c')
print('\nKPSS Statistic: %f' % result[0])
print('p-value: %f' % result[1])
for key, value in result[3].items():
    print('Critial Values:')
    print(f'   {key}, {value}')

from pandas import read_csv
from matplotlib import pyplot
from statsmodels.tsa.seasonal import seasonal_decompose

#Bitcoin Seasonal Decompose
result = seasonal_decompose(BTC_log_diff)
result.plot()
pyplot.show()

"""# MODELS"""

data =  df['2018-03-01':'2020-08-02']
data

# #Log
# data['ETHER_log'] = np.log(data['ETHER_close'])
# data['XRP_log'] = np.log(data['XRP_close'])
# data['BTC_log'] = np.log(data['BTC_close'])


#Log
data['ETHER_log'] = np.log(data['ETHER_close'])
data['XRP_log'] = np.log(data['XRP_close'])
data['BTC_log'] = np.log(data['BTC_close'])


# moving_avg = BTC_log.rolling(50).mean()
# data["BTC_log_moving_avg_diff"] = data['BTC_log'] - moving_avg

#Log Shift (diff)
data['ETHER_log'] = data['ETHER_log'] - data['ETHER_log'].shift()
data['XRP_log'] = data['XRP_log'] - data['XRP_log'].shift()
data['BTC_log'] = data['BTC_log'].shift()



data.dropna(inplace=True)

data

"""# Lag the predictor variables 7 days"""

# Duplicate the bitcoin variables before lagging
data['BTC_close_dup'] = data['BTC_log']
data

# Remove the BTC open and BTC vol variables before lagging as they  can't be used for this type of modelling strategy. 
data = data.drop(['BTC_vol', "ETHER_close","ETHER_vol",	"XRP_close",	"XRP_vol","BTC_close", "Month"], axis = 1) #"BTC_log"
data

# Lag all variables except BTC_close
data['ETHER_log'] = data['ETHER_log'].shift(7)
data['XRP_log'] = data['XRP_log'].shift(7)
data['BTC_close_dup'] = data['BTC_close_dup'].shift(7)
data

# Drop the first 7 rows of the data set as these are NA rows due to the lagging. 
data.drop(data.head(7).index, inplace=True)
data

"""# Set the test and train sets"""

# Set the  data index as a data col
#data.index = range(1,1168)

len(data)

# May train and test sets
train_may = data['2018-03-01':'2020-04-30']
test_may = data['2020-05-01':'2020-05-07']

# July train and test sets
# train_july = data[1:1132]
# test_july = data[1132:1139]

# Select the independent variables for the first experiment in 'x' and the dependent variable 'y' for MAY
x_train_may = train_may[["ETHER_log",	"XRP_log",		"BTC_close_dup"]]
y_train_may = train_may['BTC_log']

# Setup testing x and y variables
x_test_may = test_may[["ETHER_log",	"XRP_log",		"BTC_close_dup"]]
y_test_may = test_may['BTC_log']

# # Select the independent variables for the first experiment in 'x' and the dependent variable 'y' for JULY
# x_train_july = train_july[['ETHER_open', 'ETHER_close', 'ETHER_vol', 'XRP_open', 'XRP_close', 'XRP_vol', 'BTC_open_dup', 'BTC_close_dup', 'BTC_vol_dup']]
# y_train_july = train_july['BTC_close']

# # Setup testing x and y variables
# x_test_july = test_july[['ETHER_open', 'ETHER_close', 'ETHER_vol', 'XRP_open', 'XRP_close', 'XRP_vol', 'BTC_open_dup', 'BTC_close_dup', 'BTC_vol_dup']]
# y_test_july = test_july['BTC_close']

"""# May Predictions

# Linear Regression
"""

# Import the requried packages for the linear regression model
import numpy as np
from sklearn.linear_model import LinearRegression

model = LinearRegression()

# Runt he linear regressor over the x and y training data
model = LinearRegression().fit(x_train_may, y_train_may)

# Runt he linear regressor over the x and y training data
from statsmodels.api import OLS
OLS(y_train_may,x_train_may).fit().summary()

# Predict the model on the x_test data
y_pred_may = pd.DataFrame(model.predict(x_test_may))
y_pred_may['index'] = range(1,8)
y_pred_may

# y_pred_may[0]=np.exp(y_pred_may[0])
# y_pred_may

y_test_may = pd.DataFrame(y_test_may)[0:7]
y_test_may['index'] = range(1,8)
y_test_may

# merge the prediction and actual data frames
merge_may = y_test_may.merge(y_pred_may, on='index', how='left')
merge_may

merge_may.columns = ['Real', 'index', 'Linear Regression']
merge_may

from sklearn.metrics import mean_squared_error

MSE = mean_squared_error(y_true = merge_may.Real, y_pred = merge_may['Linear Regression'])
np.sqrt(MSE)

# Calculate the r_square value for the linear model
r_sq = model.score(x_train_may, y_train_may)
print('coefficient of determination:', r_sq)

# Plot the predicted vs actual in matplot lib
import matplotlib.pyplot as plt
plt.plot( 'index', 'Real', data=merge_may, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)
plt.plot( 'index', 'Linear Regression', data=merge_may, marker='', color='olive', linewidth=2)
plt.xticks(rotation='vertical')
plt.legend()

"""## Random Forest - May"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score,mean_squared_error

# Random Forest tunning 
RandomForestRegression = RandomForestRegressor(n_estimators=500)

RandomForestRegression.fit(x_train_may, y_train_may)

rf_prediction = RandomForestRegression.predict(x_test_may)
rf_prediction = pd.DataFrame(rf_prediction)
rf_prediction.columns = ['Random Forest']
rf_prediction['index'] = range(1,8)
rf_prediction

y_test_may = pd.DataFrame(y_test_may)
y_test_may['index'] = range(1,8)
y_test_may.columns = ['Real', 'index']

merge_may = y_test_may.merge(rf_prediction, on="index")

rf_mse = mean_squared_error(y_test_may, rf_prediction)
rf_mse

rf_rmse = np.sqrt(rf_mse)
rf_rmse

# Plot the predicted vs actual in matplot lib
import matplotlib.pyplot as plt
plt.plot( 'index', 'Real', data=merge_may, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)
plt.plot( 'index', 'Random Forest', data=merge_may, marker='', color='olive', linewidth=2)
plt.xticks(rotation='vertical')
plt.legend()